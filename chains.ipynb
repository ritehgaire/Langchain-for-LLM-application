{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b112ec9-0db3-4f32-93c4-04777a8437ba",
   "metadata": {},
   "source": [
    "Cell 1: Setup the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ab15ae-5d1c-4bad-b6ed-e0a4b1c5e881",
   "metadata": {},
   "source": [
    "Load environment variables and suppress warnings to ensure the environment is properly set up for the LangChain chains. Additionally, the code handles model deprecation by selecting the correct LLM model based on the current date.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58dd52d1-b096-47ac-893d-86b9516ba4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "_ = load_dotenv(find_dotenv())  # Load .env file\n",
    "\n",
    "# Suppress warnings related to deprecated functions or modules\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Handle model deprecation based on current date\n",
    "import datetime\n",
    "\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define a target date after which a new model should be used\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Conditionally set the LLM model based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a0f200-5118-4308-bbdd-01be2b69d992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd96814a-ee96-48eb-a297-3750166668c8",
   "metadata": {},
   "source": [
    "Cell 2: Load and preview data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a5a3d6-9252-4e55-8855-8e477522c2ce",
   "metadata": {},
   "source": [
    "Load a CSV file (Data.csv) using Pandas and preview the first few rows. This data is used later for sequential chains involving review translation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482a5966-9466-45e1-9d27-7aba8e6923ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pandas if not already installed\n",
    "#!pip install pandas\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Data.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525ac63d-9b4c-4f26-a04d-5446ecd1367c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e28f70e-5f65-45a7-aabe-41cb89717b3e",
   "metadata": {},
   "source": [
    "Cell 3: LLMChain example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12441cf6-0d4a-470d-a0de-71134c95c464",
   "metadata": {},
   "source": [
    "Demonstrate the use of LLMChain in LangChain by generating a product name suggestion for a given input using a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02063896-acc3-46f5-b655-0f6cf38b0c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary classes for LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Initialize the LLM with specified model and temperature\n",
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
    "\n",
    "# Define a prompt template that asks for a company name based on a product\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe a company that makes {product}?\"\n",
    ")\n",
    "\n",
    "# Create the LLMChain with the prompt and the LLM\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Define the product\n",
    "product = \"Queen Size Sheet Set\"\n",
    "\n",
    "# Run the chain with the product input\n",
    "chain.run(product)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4396ce09-a71f-4cf7-8e23-869e4486250c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bfcf48c-34b8-42ff-b07c-5fa958690a7f",
   "metadata": {},
   "source": [
    "Cell 4: SimpleSequentialChain example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb7fa0c-bb60-4019-857a-0ea1545aa985",
   "metadata": {},
   "source": [
    "Use SimpleSequentialChain to create a chain of two LLMs where the output of the first is passed as input to the second.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe326bc-7f6e-4516-8430-5b6053602d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SimpleSequentialChain class\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "# Initialize the LLM with specified model and temperature\n",
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
    "\n",
    "# Define the first prompt template (company name suggestion)\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe a company that makes {product}?\"\n",
    ")\n",
    "\n",
    "# Chain 1: Suggest company name\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)\n",
    "\n",
    "# Define the second prompt template (company description)\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a 20 words description for the following company: {company_name}\"\n",
    ")\n",
    "\n",
    "# Chain 2: Provide a description for the suggested company name\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
    "\n",
    "# Combine the two chains into a SimpleSequentialChain\n",
    "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two], verbose=True)\n",
    "\n",
    "# Run the combined chain with the product input\n",
    "overall_simple_chain.run(product)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992408c0-9e03-4b6a-bc94-557f686a9a19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8abe3e72-6bb4-4495-8ffd-a691370360bf",
   "metadata": {},
   "source": [
    "Cell 5: SequentialChain example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4d7357-0d2f-4e5d-bc4e-e1db226ca694",
   "metadata": {},
   "source": [
    "Create a SequentialChain that chains multiple LLMs together, processing inputs and outputs across multiple steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e18dd0-1627-4ffe-bb56-16b0a50a4659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SequentialChain class\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
    "\n",
    "# First chain: Translate review to English\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following review to English:\\n\\n{Review}\"\n",
    ")\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, output_key=\"English_Review\")\n",
    "\n",
    "# Second chain: Summarize the English review\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Can you summarize the following review in 1 sentence:\\n\\n{English_Review}\"\n",
    ")\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, output_key=\"summary\")\n",
    "\n",
    "# Third chain: Detect the language of the review\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What language is the following review:\\n\\n{Review}\"\n",
    ")\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt, output_key=\"language\")\n",
    "\n",
    "# Fourth chain: Write a follow-up response in the detected language\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a follow-up response to the following summary in the specified language:\"\n",
    "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
    ")\n",
    "chain_four = LLMChain(llm=llm, prompt=fourth_prompt, output_key=\"followup_message\")\n",
    "\n",
    "# Combine all chains into a SequentialChain\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"Review\"],\n",
    "    output_variables=[\"English_Review\", \"summary\", \"followup_message\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Get a review from the dataset and run the chain\n",
    "review = df.Review[5]\n",
    "overall_chain(review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647faace-4768-4577-baf1-6c5b1e600d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "289ed464-0e67-4dff-b3eb-ab572dc3ac4f",
   "metadata": {},
   "source": [
    "Cell 6: Router Chain example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880bb43a-7673-43ba-a0d1-385bc3688ebf",
   "metadata": {},
   "source": [
    "Use a RouterChain to dynamically choose between different LLM chains based on the input type (e.g., physics, math, history, or computer science questions).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8c4c33-dd72-4dbf-921a-8047565d8631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt templates for different subject areas\n",
    "physics_template = \"\"\"You are a very smart physics professor...\"\"\"\n",
    "math_template = \"\"\"You are a very good mathematician...\"\"\"\n",
    "history_template = \"\"\"You are a very good historian...\"\"\"\n",
    "computerscience_template = \"\"\"You are a successful computer scientist...\"\"\"\n",
    "\n",
    "# Define a list of prompt information\n",
    "prompt_infos = [\n",
    "    {\"name\": \"physics\", \"description\": \"Good for answering questions about physics\", \"prompt_template\": physics_template},\n",
    "    {\"name\": \"math\", \"description\": \"Good for answering math questions\", \"prompt_template\": math_template},\n",
    "    {\"name\": \"History\", \"description\": \"Good for answering history questions\", \"prompt_template\": history_template},\n",
    "    {\"name\": \"computer science\", \"description\": \"Good for answering computer science questions\", \"prompt_template\": computerscience_template}\n",
    "]\n",
    "\n",
    "# Import necessary classes for the router chain\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(temperature=0, model=llm_model)\n",
    "\n",
    "# Create destination chains for each prompt\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain\n",
    "\n",
    "# Define the default prompt for unhandled cases\n",
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)\n",
    "\n",
    "# Define the routing logic template\n",
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input...\"\"\"\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=\"\\n\".join([f\"{p['name']}: {p['description']}\" for p in prompt_infos]))\n",
    "router_prompt = PromptTemplate(template=router_template, input_variables=[\"input\"], output_parser=RouterOutputParser())\n",
    "\n",
    "# Initialize the router chain\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
    "\n",
    "# Create the final multi-prompt chain\n",
    "chain = MultiPromptChain(router_chain=router_chain, destination_chains=destination_chains, default_chain=default_chain, verbose=True)\n",
    "\n",
    "# Test the router chain with different inputs\n",
    "chain.run(\"What is black body radiation?\")\n",
    "chain.run(\"What is 2 + 2?\")\n",
    "chain.run(\"Why does every cell in our body contain DNA?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd5bc0e-a315-4fbb-a2fc-c9dbb8b914f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0991bbfe-4e1f-400c-bcbb-d8a8116649e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcefca97-53f5-4cb2-a488-92c02566bc66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d05232-42e1-428c-971e-f9dff79fc808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dd9be3-5897-4697-8e45-d431876654ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaa34b1-0bc5-4890-b5f0-5fb66d32d1c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac2f148-1d3a-4029-b5ea-aa6563853132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577c6739-cc70-4aae-919a-890462238526",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
