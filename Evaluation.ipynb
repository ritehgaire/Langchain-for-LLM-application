{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94824386-c7ce-40a2-8728-6e08b437eb8e",
   "metadata": {},
   "source": [
    "Cell 1: Setup the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbcfec1-9431-44e6-9cef-73524c6ea3ae",
   "metadata": {},
   "source": [
    "Load environment variables and select the appropriate LLM model based on the current date to ensure the use of the most updated model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7e2118-7cb5-4247-a3a6-a2ccb7811c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "_ = load_dotenv(find_dotenv())  # Load .env file\n",
    "\n",
    "# Suppress warnings related to deprecated functions or modules\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Handle model deprecation based on current date\n",
    "import datetime\n",
    "\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define a target date after which a new model should be used\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Conditionally set the LLM model based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e66f9f-ece9-4e66-8f9c-bece6c4e6ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b36c7d4-35ef-4c7a-a19f-c61ffbc98a86",
   "metadata": {},
   "source": [
    "Cell 2: Create the Q&A Application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0c318a-2894-4c74-88fe-b3023dc4dfac",
   "metadata": {},
   "source": [
    "Create a basic Q&A application using LangChain’s RetrievalQA that will allow querying a product catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d61041d-51f2-40b6-963a-a0593317907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary LangChain modules\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "# Define the path to the CSV file\n",
    "file = 'OutdoorClothingCatalog_1000.csv'\n",
    "\n",
    "# Load the product data from CSV\n",
    "loader = CSVLoader(file_path=file)\n",
    "data = loader.load()\n",
    "\n",
    "# Create a vector store index from the CSV loader\n",
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch\n",
    ").from_loaders([loader])\n",
    "\n",
    "# Initialize the language model (LLM) for Q&A\n",
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "\n",
    "# Create the RetrievalQA chain for querying\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=index.vectorstore.as_retriever(), \n",
    "    verbose=True,\n",
    "    chain_type_kwargs={\"document_separator\": \"<<<<>>>>>\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15eec60-38c7-49fc-9b2d-38fed56e4854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "deab703a-b529-4b4f-9580-d53e05185c4b",
   "metadata": {},
   "source": [
    "Cell 3: Coming Up with Test Data Points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15215d4d-926b-4d0a-844b-f650af0c6db1",
   "metadata": {},
   "source": [
    "Inspect a few data points to understand the structure of the dataset and develop hard-coded examples for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f08b382-7a25-4fbf-a05c-a827f52871fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect specific data points from the dataset\n",
    "data[10]\n",
    "data[11]\n",
    "\n",
    "# Hard-coded examples for testing\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"Do the Cozy Comfort Pullover Set have side pockets?\",\n",
    "        \"answer\": \"Yes\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What collection is the Ultra-Lofty 850 Stretch Down Hooded Jacket from?\",\n",
    "        \"answer\": \"The DownTek collection\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db9fcc3-3e1f-406f-8ce8-94bd149e6b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35a65cca-9325-4b11-a852-661f33beee7d",
   "metadata": {},
   "source": [
    "Cell 4: Generate LLM-Generated Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd2defa-7ea6-49c8-9d9d-b3b973c766bd",
   "metadata": {},
   "source": [
    "Use LangChain’s QAGenerateChain to automatically generate test examples from the dataset using the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a135b803-e7c4-405e-b0c9-a23df75f28de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the QAGenerateChain for generating examples\n",
    "from langchain.evaluation.qa import QAGenerateChain\n",
    "\n",
    "# Initialize the example generation chain using the LLM\n",
    "example_gen_chain = QAGenerateChain.from_llm(ChatOpenAI(model=llm_model))\n",
    "\n",
    "# Generate new examples from the dataset (using the first 5 entries as an example)\n",
    "# The warning about ignoring can be safely ignored\n",
    "new_examples = example_gen_chain.apply_and_parse(\n",
    "    [{\"doc\": t} for t in data[:5]]\n",
    ")\n",
    "\n",
    "# Inspect the first generated example\n",
    "new_examples[0]\n",
    "data[0]\n",
    "\n",
    "# Combine hard-coded and LLM-generated examples\n",
    "examples += new_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f8cb07-ed4e-475e-8a8b-9d072ae4dbac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06ab8ae4-6c7b-4d94-8606-65c0eab556ff",
   "metadata": {},
   "source": [
    "Cell 5: Running Q&A on an Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6972043b-df49-4e4c-af67-af5313d8ce25",
   "metadata": {},
   "source": [
    "Test the Q&A application by running a query from the examples and retrieving the result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ce6761-1b7c-48d1-82de-3dce1d7bc930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Q&A model on the first example's query\n",
    "qa.run(examples[0][\"query\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d1aabc-03fe-47f8-8a34-3252547776f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "5f4a97b9-1475-4fa2-9f2b-37e55ce0f49d",
   "metadata": {},
   "source": [
    "Cell 6: Manual Evaluation and Debuggin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505efd77-fdaf-46dd-b9c1-c13affcd8789",
   "metadata": {},
   "source": [
    "Manually evaluate and debug the system by running queries with debug mode enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acc4b83-0698-4ff5-a6d8-bdd0670d54ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable debug mode for manual evaluation\n",
    "import langchain\n",
    "langchain.debug = True\n",
    "\n",
    "# Run the Q&A model on the first example's query with debugging enabled\n",
    "qa.run(examples[0][\"query\"])\n",
    "\n",
    "# Turn off the debug mode\n",
    "langchain.debug = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99cc96b-fda5-4cc7-9265-5c0c14be31df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df0ade63-6fe8-401d-bd40-0709073e9e06",
   "metadata": {},
   "source": [
    "Cell 7: LLM-Assisted Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c489e1f-f59a-479c-8c85-9ef8284b1ba2",
   "metadata": {},
   "source": [
    "Use LangChain’s QAEvalChain to evaluate the predictions generated by the Q&A system and compare them to the correct answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cc78d1-d521-43d9-8664-1ca3aef77d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions on all examples\n",
    "predictions = qa.apply(examples)\n",
    "\n",
    "# Import QAEvalChain for LLM-assisted evaluation\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "\n",
    "# Initialize the evaluation chain using the LLM\n",
    "llm = ChatOpenAI(temperature=0, model=llm_model)\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "\n",
    "# Evaluate the predictions and grade the outputs\n",
    "graded_outputs = eval_chain.evaluate(examples, predictions)\n",
    "\n",
    "# Display the evaluation results\n",
    "for i, eg in enumerate(examples):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(\"Question: \" + predictions[i]['query'])\n",
    "    print(\"Real Answer: \" + predictions[i]['answer'])\n",
    "    print(\"Predicted Answer: \" + predictions[i]['result'])\n",
    "    print(\"Predicted Grade: \" + graded_outputs[i]['text'])\n",
    "    print()\n",
    "\n",
    "# Display the graded output for the first example\n",
    "graded_outputs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ebe1ab-a03d-490f-86f5-ff58b1222353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1743c2d-610c-45d0-95c8-4d74bec225b8",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "Cells 1-2: Set up the environment, load the product catalog data, and create a basic Q&A system using LangChain’s RetrievalQA.\n",
    "\n",
    "Cells 3-4: Test the system with both hard-coded and LLM-generated examples for better coverage of different types of queries.\n",
    "\n",
    "Cell 5: Test the Q&A system on a sample query.\n",
    "\n",
    "Cell 6: Enable debug mode to manually evaluate and inspect the internal workings of the system for better insights into its performance.\n",
    "\n",
    "Cell 7: Use LLM-assisted evaluation to automatically assess the accuracy of the Q&A system’s predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dec49c-0aa8-4d0e-a020-6af7f58b5dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dafaf62f-309b-4cfe-b184-483129bcdf43",
   "metadata": {},
   "source": [
    "Suggestions for further improvements:\n",
    "a. Extend the system with more complex multi-step queries for products that involve multiple attributes.\n",
    "b. Implement confidence scoring for each prediction to understand how certain the model is about its answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5846cf39-5114-44b2-98c8-a9928a58d7ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516921de-41c3-4532-b550-bfddcd267ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
