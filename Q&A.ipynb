{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9534bd20-3434-4c47-acb8-94ab473e0794",
   "metadata": {},
   "source": [
    "Cell 1: Setup the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bdd774-d25e-49de-b17a-97a999913640",
   "metadata": {},
   "source": [
    "Set up environment variables and suppress warnings. Account for the deprecation of specific LLM models by selecting the appropriate model based on the current date.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7e31e2-95bc-436a-87bc-11b97a48b8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "_ = load_dotenv(find_dotenv())  # Load .env file\n",
    "\n",
    "# Suppress warnings related to deprecated functions or modules\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Handle model deprecation based on current date\n",
    "import datetime\n",
    "\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define a target date after which a new model should be used\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Conditionally set the LLM model based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bd27d0-ca66-4cbe-acd1-e8fa731048f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "645ac5a5-15d4-4bbb-82f6-a99cb148122a",
   "metadata": {},
   "source": [
    "Cell 2: Load the product catalog from CSV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f9d087-3a57-44bf-8d24-cff0b2e165fa",
   "metadata": {},
   "source": [
    "Load the product catalog from a CSV file using CSVLoader, which reads the file and converts it into a format that LangChain can process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191f481d-d015-4a88-8b9b-930280a81a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary classes for document loading and vector search\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "# Define the path to the product catalog file\n",
    "file = 'OutdoorClothingCatalog_1000.csv'\n",
    "\n",
    "# Load the CSV file into a document format\n",
    "loader = CSVLoader(file_path=file)\n",
    "\n",
    "# Load the documents into a variable\n",
    "docs = loader.load()\n",
    "\n",
    "# Display the first document to understand its structure\n",
    "docs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8203712-dd85-44e5-a4c4-88c9588294b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fc566c9-0430-43cb-9f0b-e8f5de3ec11f",
   "metadata": {},
   "source": [
    "Cell 3: Create embeddings for documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a737d7ee-c014-45a9-a54c-867761f49227",
   "metadata": {},
   "source": [
    "Create embeddings for the documents using OpenAI embeddings, which will be used to perform similarity searches over the documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4191cac3-2dbe-4ea3-b36e-6911a00db3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OpenAI embeddings class\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Initialize embeddings using OpenAI's API\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Embed a query as an example\n",
    "embed = embeddings.embed_query(\"Hi my name is Harrison\")\n",
    "\n",
    "# Check the length of the embedding and display the first 5 values\n",
    "print(len(embed))\n",
    "print(embed[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f282938-43cd-450e-bbbb-a5a4f6941d50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdeb44e4-d9b9-475d-9c0a-7a69266932d3",
   "metadata": {},
   "source": [
    "Cell 4: Create a document search database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dfc43e-e5db-4a10-bd5e-58b6bb200537",
   "metadata": {},
   "source": [
    "Use DocArrayInMemorySearch to create a database from the documents and embeddings. This allows performing similarity-based searches on the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec88460-3fbf-4893-88af-f03883fa406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use DocArrayInMemorySearch to create an in-memory search database with documents and embeddings\n",
    "db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n",
    "\n",
    "# Perform a similarity search based on a query related to sunblocking shirts\n",
    "query = \"Please suggest a shirt with sunblocking\"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "# Check the number of matching documents\n",
    "len(docs)\n",
    "\n",
    "# Display the first document in the results\n",
    "docs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eccf010-7c40-4560-8d20-13641237b2e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a376a646-1de6-4c51-b217-c9bb1bdac468",
   "metadata": {},
   "source": [
    "Cell 5: Setup LLM-based Q&A over documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1403d8f-d8a4-4251-b889-7fe22d0ae0ea",
   "metadata": {},
   "source": [
    "Use RetrievalQA to perform a question-answering process over the document database using the retrieved documents and a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec35419-fec2-4048-9f14-c55871268dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the document database to a retriever to enable querying\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# Initialize the language model\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "\n",
    "# Combine the content of all retrieved documents\n",
    "qdocs = \"\".join([docs[i].page_content for i in range(len(docs))])\n",
    "\n",
    "# Use the LLM to answer the question based on the retrieved documents\n",
    "response = llm.call_as_llm(f\"{qdocs} Question: Please list all your shirts with sun protection in a table in markdown and summarize each one.\")\n",
    "\n",
    "# Display the response in Markdown format\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64f1252-78af-44b3-b7b3-69c8e3608b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94cff238-c5af-4072-a3bd-03e62dc10885",
   "metadata": {},
   "source": [
    "Cell 6: Use RetrievalQA for question answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906b285c-2d26-4935-aab3-edf6032d8afb",
   "metadata": {},
   "source": [
    "Set up RetrievalQA using the \"stuff\" chain type, where the chain retrieves relevant documents and uses the LLM to answer queries based on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49db3826-1cff-4b7c-803f-e8347f89383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the RetrievalQA class\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Setup the RetrievalQA chain using \"stuff\" as the chain type\n",
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Define the query for shirts with sun protection\n",
    "query = \"Please list all your shirts with sun protection in a table in markdown and summarize each one.\"\n",
    "\n",
    "# Run the query and display the response in markdown format\n",
    "response = qa_stuff.run(query)\n",
    "display(Markdown(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea395a9e-0ef3-4ecd-b745-f3ba1b7da062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce71fb61-65c5-4cfa-83d3-75b5df8a48fd",
   "metadata": {},
   "source": [
    "Cell 7: VectorstoreIndexCreator to index and query documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2781902-bfe5-4636-b28b-55ee9617f7eb",
   "metadata": {},
   "source": [
    "Use VectorstoreIndexCreator to create a vector search index over the documents, and then query it for relevant information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e8958c-4624-4952-94f8-c9ce8b783a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import VectorstoreIndexCreator and necessary classes\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Create an index using VectorstoreIndexCreator\n",
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch\n",
    ").from_loaders([loader])\n",
    "\n",
    "# Define the query for the index\n",
    "query = \"Please list all your shirts with sun protection in a table in markdown and summarize each one.\"\n",
    "\n",
    "# Set up the LLM replacement model (due to model deprecation)\n",
    "llm_replacement_model = OpenAI(temperature=0, model='gpt-3.5-turbo-instruct')\n",
    "\n",
    "# Query the index and display the response\n",
    "response = index.query(query, llm=llm_replacement_model)\n",
    "display(Markdown(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7009d7e7-2e77-404f-b2b0-1cefc8e9520b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0dad9ad6-e97f-434a-9b6f-44f93255728c",
   "metadata": {},
   "source": [
    "Suggestions for further improvements:\n",
    "a. Extend the question-answering functionality to handle multi-step queries across different product categories.\n",
    "b. Add caching for embeddings to avoid recalculating them every time a query is made.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67decc95-9afb-49b6-92ed-103c7770649b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccef57d4-cf1b-4e9a-8836-b63fb9b92694",
   "metadata": {},
   "source": [
    "Cell 8: Extend Q&A for Multi-Step Queries across Different Product Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caa0466-61ac-457c-a17d-b9eb2ad3fa8d",
   "metadata": {},
   "source": [
    "Extend the functionality to handle multi-step queries, allowing the user to ask about different product categories (e.g., shirts, pants, jackets) in a single conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821523cc-a301-4dcb-baed-b0951ebec208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a multi-step query function for different product categories\n",
    "def multi_step_query(categories, retriever, llm):\n",
    "    responses = {}\n",
    "    \n",
    "    # Iterate through each product category and run a query\n",
    "    for category in categories:\n",
    "        query = f\"Please list all your {category} with sun protection in a table in markdown and summarize each one.\"\n",
    "        response = qa_stuff.run(query)\n",
    "        responses[category] = response\n",
    "    \n",
    "    return responses\n",
    "\n",
    "# Example categories to query for\n",
    "categories = [\"shirts\", \"pants\", \"jackets\"]\n",
    "\n",
    "# Run the multi-step query function\n",
    "responses = multi_step_query(categories, retriever, llm)\n",
    "\n",
    "# Display the results for each category in markdown format\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "for category, response in responses.items():\n",
    "    print(f\"Category: {category}\")\n",
    "    display(Markdown(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290d800d-ee52-4637-93e8-ae9ffd9b3b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb603fbb-cf1a-44ec-92c3-5c2a4ee6e7f2",
   "metadata": {},
   "source": [
    "Cell 9: Add Caching for Embeddings to Avoid Recalculating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c1568a-8d17-44b7-9f12-a9de24da6e57",
   "metadata": {},
   "source": [
    "Implement a caching mechanism for embeddings to avoid recalculating them every time a query is made, improving efficiency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e1d74e-cd97-493f-a7a2-41e445ccfb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules for caching\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Define the file path for cached embeddings\n",
    "cache_file = \"embeddings_cache.pkl\"\n",
    "\n",
    "# Function to load embeddings from cache if available\n",
    "def load_embeddings_cache():\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            cached_db = pickle.load(f)\n",
    "        print(\"Loaded embeddings from cache.\")\n",
    "        return cached_db\n",
    "    return None\n",
    "\n",
    "# Function to save embeddings to cache\n",
    "def save_embeddings_cache(db):\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(db, f)\n",
    "    print(\"Embeddings saved to cache.\")\n",
    "\n",
    "# Load cached embeddings if they exist\n",
    "cached_db = load_embeddings_cache()\n",
    "\n",
    "# If no cached embeddings, calculate and cache them\n",
    "if cached_db is None:\n",
    "    db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n",
    "    save_embeddings_cache(db)\n",
    "else:\n",
    "    db = cached_db\n",
    "\n",
    "# Continue with similarity search and Q&A process using cached or newly created embeddings\n",
    "query = \"Please suggest a shirt with sunblocking\"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "# Display the first matching document\n",
    "docs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb455dc-44d3-4449-a663-531011b6a982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7df7b8f8-61ff-426e-8012-298be3f0f07f",
   "metadata": {},
   "source": [
    "Suggestions for further improvements:\n",
    "a. Add logging to track how many queries are made across different categories.\n",
    "b. Implement a more sophisticated cache invalidation strategy to handle updated data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957df4d1-6579-4da7-be02-163015fb6d38",
   "metadata": {},
   "source": [
    "Cell 10: Add Logging to Track Queries Across Different Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc4ef86-ccfd-49b5-a23e-024dbef7f999",
   "metadata": {},
   "source": [
    "Implement logging to track how many queries are made for each product category, along with timestamps and the details of the queries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05624e53-87f7-4b17-b5f5-31d2da05073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the logging module\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging to write to a file with timestamps\n",
    "logging.basicConfig(filename='query_log.log', \n",
    "                    level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Function to log query details\n",
    "def log_query(category, query):\n",
    "    logging.info(f\"Query made for category: {category} | Query: {query}\")\n",
    "\n",
    "# Modify the multi-step query function to include logging\n",
    "def multi_step_query_with_logging(categories, retriever, llm):\n",
    "    responses = {}\n",
    "    \n",
    "    # Iterate through each product category and run a query\n",
    "    for category in categories:\n",
    "        query = f\"Please list all your {category} with sun protection in a table in markdown and summarize each one.\"\n",
    "        \n",
    "        # Log the query before executing it\n",
    "        log_query(category, query)\n",
    "        \n",
    "        # Run the query and store the response\n",
    "        response = qa_stuff.run(query)\n",
    "        responses[category] = response\n",
    "    \n",
    "    return responses\n",
    "\n",
    "# Example categories to query for\n",
    "categories = [\"shirts\", \"pants\", \"jackets\"]\n",
    "\n",
    "# Run the multi-step query with logging\n",
    "responses = multi_step_query_with_logging(categories, retriever, llm)\n",
    "\n",
    "# Display the results for each category in markdown format\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "for category, response in responses.items():\n",
    "    print(f\"Category: {category}\")\n",
    "    display(Markdown(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d00350-be54-41a6-9a00-ee7f164aecda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec2d6184-b03d-4c74-9657-ef292547dca5",
   "metadata": {},
   "source": [
    "Cell 11: Implement a More Sophisticated Cache Invalidation Strategy"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f1d33fa-4c19-4ead-b4e6-9eda1516f6f6",
   "metadata": {},
   "source": [
    "Implement a cache invalidation strategy that invalidates (i.e., removes or updates) the cache if the source document has been modified since the cache was last generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f30c37c-57cc-45e8-859b-9a1fe729df21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os module to check file modification time\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Define the cache file path and the document source file path\n",
    "cache_file = \"embeddings_cache.pkl\"\n",
    "document_file = \"OutdoorClothingCatalog_1000.csv\"\n",
    "\n",
    "# Function to check if cache needs invalidation (i.e., if the document has been modified since cache was created)\n",
    "def is_cache_valid(cache_file, document_file):\n",
    "    # Check if cache file exists\n",
    "    if not os.path.exists(cache_file):\n",
    "        return False  # Cache is not valid if it doesn't exist\n",
    "\n",
    "    # Get the modification time of the document and cache files\n",
    "    document_mtime = os.path.getmtime(document_file)\n",
    "    cache_mtime = os.path.getmtime(cache_file)\n",
    "\n",
    "    # Cache is invalid if the document has been modified after the cache was created\n",
    "    return cache_mtime > document_mtime\n",
    "\n",
    "# Function to load cache if valid, or regenerate cache if invalid\n",
    "def load_or_regenerate_cache(docs, embeddings):\n",
    "    if is_cache_valid(cache_file, document_file):\n",
    "        print(\"Cache is valid. Loading embeddings from cache.\")\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            cached_db = pickle.load(f)\n",
    "        return cached_db\n",
    "    else:\n",
    "        print(\"Cache is invalid or missing. Regenerating embeddings.\")\n",
    "        db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n",
    "        save_embeddings_cache(db)  # Save new cache after regenerating\n",
    "        return db\n",
    "\n",
    "# Function to save embeddings to cache\n",
    "def save_embeddings_cache(db):\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(db, f)\n",
    "    print(\"Embeddings saved to cache.\")\n",
    "\n",
    "# Load documents and embeddings, either from cache or regenerate if invalid\n",
    "db = load_or_regenerate_cache(docs, embeddings)\n",
    "\n",
    "# Continue with similarity search and Q&A process using the cache or newly created embeddings\n",
    "query = \"Please suggest a shirt with sunblocking\"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "# Display the first matching document\n",
    "docs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f405e801-46af-4830-85fd-57417dbd076c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab8da684-def0-4738-bf4f-d64b40bec9dc",
   "metadata": {},
   "source": [
    "Suggestions for further improvements:\n",
    "a. Add error handling to ensure robustness in case of missing files or corrupted caches.\n",
    "b. Implement periodic cache expiration to refresh embeddings periodically, even if the source document hasn't changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf113c7-31b9-41fc-acc5-d9d0a7b41e41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34670a07-39fd-4330-9624-459628e6b1aa",
   "metadata": {},
   "source": [
    "Cell 12: Add Error Handling for Missing Files or Corrupted Caches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9955da-064d-4e80-8085-4d79b00e9499",
   "metadata": {},
   "source": [
    "Add error handling to manage situations where files (such as cache or source document) are missing or the cache is corrupted, ensuring the program continues to function correctly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1562f8-17e6-47eb-ab42-adf998ceaf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Function to load cache with error handling for missing or corrupted files\n",
    "def load_embeddings_cache_with_error_handling():\n",
    "    try:\n",
    "        if os.path.exists(cache_file):\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                cached_db = pickle.load(f)\n",
    "            print(\"Loaded embeddings from cache.\")\n",
    "            return cached_db\n",
    "        else:\n",
    "            print(\"Cache file not found.\")\n",
    "            return None\n",
    "    except (pickle.UnpicklingError, EOFError):\n",
    "        print(\"Cache file is corrupted. Regenerating embeddings.\")\n",
    "        return None\n",
    "\n",
    "# Function to save embeddings to cache with error handling\n",
    "def save_embeddings_cache_with_error_handling(db):\n",
    "    try:\n",
    "        with open(cache_file, 'wb') as f:\n",
    "            pickle.dump(db, f)\n",
    "        print(\"Embeddings saved to cache.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save embeddings to cache: {e}\")\n",
    "\n",
    "# Function to regenerate embeddings if cache is missing or invalid\n",
    "def load_or_regenerate_cache_with_error_handling(docs, embeddings):\n",
    "    try:\n",
    "        if is_cache_valid(cache_file, document_file):\n",
    "            print(\"Cache is valid. Loading embeddings from cache.\")\n",
    "            return load_embeddings_cache_with_error_handling()\n",
    "        else:\n",
    "            print(\"Cache is invalid or missing. Regenerating embeddings.\")\n",
    "            db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n",
    "            save_embeddings_cache_with_error_handling(db)  # Save new cache\n",
    "            return db\n",
    "    except Exception as e:\n",
    "        print(f\"Error during cache handling: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load documents and embeddings with error handling\n",
    "db = load_or_regenerate_cache_with_error_handling(docs, embeddings)\n",
    "\n",
    "# If db is None, ensure the program doesn't crash\n",
    "if db is not None:\n",
    "    # Continue with similarity search and Q&A process using the cache or newly created embeddings\n",
    "    query = \"Please suggest a shirt with sunblocking\"\n",
    "    docs = db.similarity_search(query)\n",
    "\n",
    "    # Display the first matching document\n",
    "    docs[0]\n",
    "else:\n",
    "    print(\"Unable to proceed due to cache or document issues.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d317f5d-0a30-4672-97b4-1d3ddbd29a04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8679be2-4e38-4a20-9476-6e313982c721",
   "metadata": {},
   "source": [
    "Cell 13: Implement Periodic Cache Expiration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9301236-440c-4a1a-a372-0b3acff58233",
   "metadata": {},
   "source": [
    "Implement periodic cache expiration to refresh embeddings periodically, even if the source document hasn't changed, ensuring the cache stays updated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ddaec9-99da-4373-9570-022b844e6d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Define the cache expiration time in seconds (e.g., 7 days = 7 * 24 * 60 * 60)\n",
    "CACHE_EXPIRATION_TIME = 7 * 24 * 60 * 60  # 7 days\n",
    "\n",
    "# Function to check if the cache is expired based on last modification time\n",
    "def is_cache_expired(cache_file):\n",
    "    if os.path.exists(cache_file):\n",
    "        cache_mtime = os.path.getmtime(cache_file)\n",
    "        current_time = time.time()\n",
    "        # Check if cache is older than expiration time\n",
    "        return (current_time - cache_mtime) > CACHE_EXPIRATION_TIME\n",
    "    return True  # If cache doesn't exist, consider it expired\n",
    "\n",
    "# Function to check cache validity, combining document modification and expiration logic\n",
    "def is_cache_valid_with_expiration(cache_file, document_file):\n",
    "    # Check if cache file exists and is not expired\n",
    "    if is_cache_expired(cache_file):\n",
    "        print(\"Cache is expired. Regenerating embeddings.\")\n",
    "        return False\n",
    "\n",
    "    # Also ensure document has not been modified since cache was created\n",
    "    document_mtime = os.path.getmtime(document_file)\n",
    "    cache_mtime = os.path.getmtime(cache_file)\n",
    "\n",
    "    return cache_mtime > document_mtime\n",
    "\n",
    "# Regenerate cache if it's expired or invalid\n",
    "def load_or_regenerate_cache_with_expiration(docs, embeddings):\n",
    "    if is_cache_valid_with_expiration(cache_file, document_file):\n",
    "        print(\"Cache is valid and not expired. Loading embeddings from cache.\")\n",
    "        return load_embeddings_cache_with_error_handling()\n",
    "    else:\n",
    "        print(\"Cache is invalid or expired. Regenerating embeddings.\")\n",
    "        db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n",
    "        save_embeddings_cache_with_error_handling(db)  # Save new cache\n",
    "        return db\n",
    "\n",
    "# Load documents and embeddings with expiration handling\n",
    "db = load_or_regenerate_cache_with_expiration(docs, embeddings)\n",
    "\n",
    "# If db is None, ensure the program doesn't crash\n",
    "if db is not None:\n",
    "    # Continue with similarity search and Q&A process using the cache or newly created embeddings\n",
    "    query = \"Please suggest a shirt with sunblocking\"\n",
    "    docs = db.similarity_search(query)\n",
    "\n",
    "    # Display the first matching document\n",
    "    docs[0]\n",
    "else:\n",
    "    print(\"Unable to proceed due to cache or document issues.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ff5d3f-b1db-46e2-9c52-87b0562c989a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44a6e22d-72fb-4934-94d2-75d679b88233",
   "metadata": {},
   "source": [
    "Suggestions for further improvements:\n",
    "a. Implement notifications or warnings when the cache is regenerated due to expiration.\n",
    "b. Add parallel processing for embedding generation to speed up the regeneration process if the document is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2339028-2c6d-4ac8-9c51-559ad71a7225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bc1cbfa-3d71-4ebb-afe9-937979524145",
   "metadata": {},
   "source": [
    "Cell 14: Implement Notifications or Warnings When Cache is Regenerated Due to Expiration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f38547-d8fd-4e38-a064-c7276c15a631",
   "metadata": {},
   "source": [
    "Add notifications (or warnings) to alert when the cache is expired and regenerated, providing clear feedback to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e75a72f-29e0-4496-9eaa-b36425e0047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Function to notify when the cache is being regenerated due to expiration\n",
    "def notify_cache_regeneration(reason):\n",
    "    warnings.warn(f\"Cache is being regenerated due to {reason}.\", UserWarning)\n",
    "\n",
    "# Function to check cache validity with expiration and notify if it's invalid\n",
    "def is_cache_valid_with_notification(cache_file, document_file):\n",
    "    # Check if cache file exists and is not expired\n",
    "    if is_cache_expired(cache_file):\n",
    "        notify_cache_regeneration(\"expiration\")\n",
    "        return False\n",
    "\n",
    "    # Ensure document has not been modified since cache was created\n",
    "    document_mtime = os.path.getmtime(document_file)\n",
    "    cache_mtime = os.path.getmtime(cache_file)\n",
    "\n",
    "    if document_mtime > cache_mtime:\n",
    "        notify_cache_regeneration(\"document modification\")\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# Regenerate cache if it's expired or invalid with notification\n",
    "def load_or_regenerate_cache_with_notification(docs, embeddings):\n",
    "    if is_cache_valid_with_notification(cache_file, document_file):\n",
    "        print(\"Cache is valid and not expired. Loading embeddings from cache.\")\n",
    "        return load_embeddings_cache_with_error_handling()\n",
    "    else:\n",
    "        print(\"Cache is invalid or expired. Regenerating embeddings.\")\n",
    "        db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n",
    "        save_embeddings_cache_with_error_handling(db)  # Save new cache\n",
    "        return db\n",
    "\n",
    "# Load documents and embeddings with notification handling\n",
    "db = load_or_regenerate_cache_with_notification(docs, embeddings)\n",
    "\n",
    "# If db is None, ensure the program doesn't crash\n",
    "if db is not None:\n",
    "    # Continue with similarity search and Q&A process using the cache or newly created embeddings\n",
    "    query = \"Please suggest a shirt with sunblocking\"\n",
    "    docs = db.similarity_search(query)\n",
    "\n",
    "    # Display the first matching document\n",
    "    docs[0]\n",
    "else:\n",
    "    print(\"Unable to proceed due to cache or document issues.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d6e3c3-edad-4610-b716-5c0d3d6db3e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f536513-0426-4a1d-bc56-e3e17d4d4bed",
   "metadata": {},
   "source": [
    "Cell 15: Add Parallel Processing for Embedding Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fda8f5d-bcdb-4a5e-a053-7960e8f8c96b",
   "metadata": {},
   "source": [
    "Implement parallel processing using Python's concurrent.futures module to speed up the embedding generation process for large documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5303d4-0456-43f5-a7ae-01d7a6c072b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "# Function to generate embeddings in parallel for each document\n",
    "def generate_embeddings_in_parallel(docs, embeddings):\n",
    "    def generate_single_embedding(doc):\n",
    "        return embeddings.embed_query(doc.page_content)\n",
    "\n",
    "    # Use ThreadPoolExecutor for parallel processing\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Map the documents to embedding generation, parallelizing the process\n",
    "        results = list(executor.map(generate_single_embedding, docs))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to regenerate the document search database with parallel embedding generation\n",
    "def regenerate_database_with_parallel_processing(docs, embeddings):\n",
    "    print(\"Generating embeddings in parallel...\")\n",
    "    \n",
    "    # Generate embeddings in parallel for all documents\n",
    "    embedded_docs = generate_embeddings_in_parallel(docs, embeddings)\n",
    "    \n",
    "    # Create a document search database using the embeddings\n",
    "    db = DocArrayInMemorySearch.from_embeddings(embedded_docs, docs)\n",
    "    \n",
    "    return db\n",
    "\n",
    "# Function to load or regenerate cache with parallel processing for large documents\n",
    "def load_or_regenerate_cache_with_parallel_processing(docs, embeddings):\n",
    "    if is_cache_valid_with_notification(cache_file, document_file):\n",
    "        print(\"Cache is valid and not expired. Loading embeddings from cache.\")\n",
    "        return load_embeddings_cache_with_error_handling()\n",
    "    else:\n",
    "        print(\"Cache is invalid or expired. Regenerating embeddings with parallel processing.\")\n",
    "        db = regenerate_database_with_parallel_processing(docs, embeddings)\n",
    "        save_embeddings_cache_with_error_handling(db)  # Save new cache\n",
    "        return db\n",
    "\n",
    "# Load documents and embeddings with parallel processing and notifications\n",
    "db = load_or_regenerate_cache_with_parallel_processing(docs, embeddings)\n",
    "\n",
    "# If db is None, ensure the program doesn't crash\n",
    "if db is not None:\n",
    "    # Continue with similarity search and Q&A process using the cache or newly created embeddings\n",
    "    query = \"Please suggest a shirt with sunblocking\"\n",
    "    docs = db.similarity_search(query)\n",
    "\n",
    "    # Display the first matching document\n",
    "    docs[0]\n",
    "else:\n",
    "    print(\"Unable to proceed due to cache or document issues.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029d7322-c0e9-44c7-b6d3-54b05c828665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2597bd82-a3e6-49b7-aeb3-a01e7f51f0d2",
   "metadata": {},
   "source": [
    "Suggestions for further improvements:\n",
    "a. Add a progress bar to track the embedding generation process for large datasets.\n",
    "b. Implement a fallback to sequential processing if parallel processing fails due to resource constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d96aa7-a38d-40ee-b21b-e3e84a22af74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55f08884-642f-470d-aa87-aefe22ebed50",
   "metadata": {},
   "source": [
    "Cell 16: Add a Progress Bar to Track Embedding Generation for Large Datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e28b97f-cbdd-4ed8-b723-afa0c7028a40",
   "metadata": {},
   "source": [
    "Integrate a progress bar to track the progress of embedding generation using tqdm, which is helpful for monitoring the process on large datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3160a20a-86bc-4eab-aea0-f1cd7f2e6302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tqdm if not already installed\n",
    "# !pip install tqdm\n",
    "\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "\n",
    "# Function to generate embeddings in parallel with progress tracking\n",
    "def generate_embeddings_with_progress(docs, embeddings):\n",
    "    def generate_single_embedding(doc):\n",
    "        return embeddings.embed_query(doc.page_content)\n",
    "\n",
    "    # Initialize tqdm progress bar\n",
    "    with tqdm(total=len(docs), desc=\"Generating embeddings\") as pbar:\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            # Use parallel processing and track progress using tqdm\n",
    "            futures = [executor.submit(generate_single_embedding, doc) for doc in docs]\n",
    "            results = []\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                results.append(future.result())\n",
    "                pbar.update(1)  # Update progress bar for each completed task\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to regenerate document search database with progress bar\n",
    "def regenerate_database_with_progress(docs, embeddings):\n",
    "    print(\"Generating embeddings in parallel with progress tracking...\")\n",
    "    \n",
    "    # Generate embeddings with progress bar\n",
    "    embedded_docs = generate_embeddings_with_progress(docs, embeddings)\n",
    "    \n",
    "    # Create document search database using the embeddings\n",
    "    db = DocArrayInMemorySearch.from_embeddings(embedded_docs, docs)\n",
    "    \n",
    "    return db\n",
    "\n",
    "# Regenerate cache with progress bar\n",
    "def load_or_regenerate_cache_with_progress(docs, embeddings):\n",
    "    if is_cache_valid_with_notification(cache_file, document_file):\n",
    "        print(\"Cache is valid and not expired. Loading embeddings from cache.\")\n",
    "        return load_embeddings_cache_with_error_handling()\n",
    "    else:\n",
    "        print(\"Cache is invalid or expired. Regenerating embeddings with progress tracking.\")\n",
    "        db = regenerate_database_with_progress(docs, embeddings)\n",
    "        save_embeddings_cache_with_error_handling(db)  # Save new cache\n",
    "        return db\n",
    "\n",
    "# Load documents and embeddings with progress tracking\n",
    "db = load_or_regenerate_cache_with_progress(docs, embeddings)\n",
    "\n",
    "# If db is None, ensure the program doesn't crash\n",
    "if db is not None:\n",
    "    # Continue with similarity search and Q&A process using the cache or newly created embeddings\n",
    "    query = \"Please suggest a shirt with sunblocking\"\n",
    "    docs = db.similarity_search(query)\n",
    "\n",
    "    # Display the first matching document\n",
    "    docs[0]\n",
    "else:\n",
    "    print(\"Unable to proceed due to cache or document issues.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e58ef47-ff00-4cd6-8b29-f3630911cfc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2f71461-4273-47c0-a19c-97b3f8b42ccd",
   "metadata": {},
   "source": [
    "Cell 17: Implement a Fallback to Sequential Processing if Parallel Processing Fails\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfa6170-6f16-4263-8ab0-8ec97e23a800",
   "metadata": {},
   "source": [
    "Add error handling to switch from parallel processing to sequential processing if parallel processing fails due to resource constraints.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0720f445-4917-40af-98b9-fc50b67d444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate embeddings sequentially as a fallback\n",
    "def generate_embeddings_sequentially(docs, embeddings):\n",
    "    print(\"Parallel processing failed. Falling back to sequential embedding generation...\")\n",
    "    \n",
    "    # Sequential embedding generation with progress bar\n",
    "    embedded_docs = []\n",
    "    for doc in tqdm(docs, desc=\"Generating embeddings sequentially\"):\n",
    "        embedded_docs.append(embeddings.embed_query(doc.page_content))\n",
    "    \n",
    "    return embedded_docs\n",
    "\n",
    "# Function to generate embeddings with fallback to sequential processing\n",
    "def generate_embeddings_with_fallback(docs, embeddings):\n",
    "    try:\n",
    "        # Try generating embeddings in parallel\n",
    "        return generate_embeddings_with_progress(docs, embeddings)\n",
    "    except (RuntimeError, concurrent.futures.ProcessPoolExecutor):\n",
    "        # On failure, fallback to sequential processing\n",
    "        return generate_embeddings_sequentially(docs, embeddings)\n",
    "\n",
    "# Function to regenerate document search database with fallback mechanism\n",
    "def regenerate_database_with_fallback(docs, embeddings):\n",
    "    print(\"Generating embeddings with fallback in case of failure...\")\n",
    "    \n",
    "    # Generate embeddings with fallback mechanism\n",
    "    embedded_docs = generate_embeddings_with_fallback(docs, embeddings)\n",
    "    \n",
    "    # Create document search database using the embeddings\n",
    "    db = DocArrayInMemorySearch.from_embeddings(embedded_docs, docs)\n",
    "    \n",
    "    return db\n",
    "\n",
    "# Regenerate cache with fallback to sequential processing\n",
    "def load_or_regenerate_cache_with_fallback(docs, embeddings):\n",
    "    if is_cache_valid_with_notification(cache_file, document_file):\n",
    "        print(\"Cache is valid and not expired. Loading embeddings from cache.\")\n",
    "        return load_embeddings_cache_with_error_handling()\n",
    "    else:\n",
    "        print(\"Cache is invalid or expired. Regenerating embeddings with fallback mechanism.\")\n",
    "        db = regenerate_database_with_fallback(docs, embeddings)\n",
    "        save_embeddings_cache_with_error_handling(db)  # Save new cache\n",
    "        return db\n",
    "\n",
    "# Load documents and embeddings with fallback mechanism\n",
    "db = load_or_regenerate_cache_with_fallback(docs, embeddings)\n",
    "\n",
    "# If db is None, ensure the program doesn't crash\n",
    "if db is not None:\n",
    "    # Continue with similarity search and Q&A process using the cache or newly created embeddings\n",
    "    query = \"Please suggest a shirt with sunblocking\"\n",
    "    docs = db.similarity_search(query)\n",
    "\n",
    "    # Display the first matching document\n",
    "    docs[0]\n",
    "else:\n",
    "    print(\"Unable to proceed due to cache or document issues.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc90f0b-1eb9-4c7d-9cd8-a70153288dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "140f1110-1f7b-436d-8419-44cccf4be218",
   "metadata": {},
   "source": [
    "Suggestions for further improvements:\n",
    "a. Add detailed logging for parallel vs. sequential processing to analyze performance trade-offs.\n",
    "b. Implement asynchronous handling to allow the embedding process to continue without blocking other operations in the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac0ea10-3cee-4026-add9-ad70e50a3d68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf9313da-ccc3-4701-9847-aac3b7581555",
   "metadata": {},
   "source": [
    "Cell 18: Add Detailed Logging for Parallel vs. Sequential Processing to Analyze Performance Trade-offs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5392816-d5ce-4c63-94b8-d85639aaaba9",
   "metadata": {},
   "source": [
    "Log detailed information about the performance (time taken) for parallel and sequential processing, allowing analysis of trade-offs between both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b900ad-5bb4-4e64-ac8b-a4b35bcbd106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "\n",
    "# Configure logging to track processing times\n",
    "logging.basicConfig(filename='processing_time.log', \n",
    "                    level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Function to log the processing time for parallel or sequential processing\n",
    "def log_processing_time(method, start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    logging.info(f\"Processing method: {method} | Time taken: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Function to generate embeddings in parallel with logging\n",
    "def generate_embeddings_with_logging(docs, embeddings):\n",
    "    start_time = time.time()  # Record start time\n",
    "    try:\n",
    "        # Try generating embeddings in parallel with progress\n",
    "        embedded_docs = generate_embeddings_with_progress(docs, embeddings)\n",
    "        end_time = time.time()  # Record end time\n",
    "        log_processing_time(\"Parallel Processing\", start_time, end_time)\n",
    "        return embedded_docs\n",
    "    except (RuntimeError, concurrent.futures.ProcessPoolExecutor):\n",
    "        # On failure, log and fallback to sequential processing\n",
    "        end_time = time.time()  # Record end time for parallel failure\n",
    "        log_processing_time(\"Parallel Processing (Failed)\", start_time, end_time)\n",
    "\n",
    "        # Switch to sequential processing\n",
    "        start_time = time.time()  # Record start time for sequential\n",
    "        embedded_docs = generate_embeddings_sequentially(docs, embeddings)\n",
    "        end_time = time.time()  # Record end time for sequential\n",
    "        log_processing_time(\"Sequential Processing\", start_time, end_time)\n",
    "        return embedded_docs\n",
    "\n",
    "# Function to regenerate document search database with logging\n",
    "def regenerate_database_with_logging(docs, embeddings):\n",
    "    print(\"Generating embeddings with detailed logging...\")\n",
    "    \n",
    "    # Generate embeddings with fallback and logging\n",
    "    embedded_docs = generate_embeddings_with_logging(docs, embeddings)\n",
    "    \n",
    "    # Create document search database using the embeddings\n",
    "    db = DocArrayInMemorySearch.from_embeddings(embedded_docs, docs)\n",
    "    \n",
    "    return db\n",
    "\n",
    "# Regenerate cache with logging for performance analysis\n",
    "def load_or_regenerate_cache_with_logging(docs, embeddings):\n",
    "    if is_cache_valid_with_notification(cache_file, document_file):\n",
    "        print(\"Cache is valid and not expired. Loading embeddings from cache.\")\n",
    "        return load_embeddings_cache_with_error_handling()\n",
    "    else:\n",
    "        print(\"Cache is invalid or expired. Regenerating embeddings with detailed logging.\")\n",
    "        db = regenerate_database_with_logging(docs, embeddings)\n",
    "        save_embeddings_cache_with_error_handling(db)  # Save new cache\n",
    "        return db\n",
    "\n",
    "# Load documents and embeddings with logging\n",
    "db = load_or_regenerate_cache_with_logging(docs, embeddings)\n",
    "\n",
    "# If db is None, ensure the program doesn't crash\n",
    "if db is not None:\n",
    "    # Continue with similarity search and Q&A process using the cache or newly created embeddings\n",
    "    query = \"Please suggest a shirt with sunblocking\"\n",
    "    docs = db.similarity_search(query)\n",
    "\n",
    "    # Display the first matching document\n",
    "    docs[0]\n",
    "else:\n",
    "    print(\"Unable to proceed due to cache or document issues.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481ed701-d25c-4cbc-a1d3-e6eb82d05b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40a561c4-bffd-4340-84c5-081281cde29f",
   "metadata": {},
   "source": [
    "Cell 19: Implement Asynchronous Handling to Allow Non-blocking Embedding Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6fa3fb-2e32-4658-97c8-9113d69e4078",
   "metadata": {},
   "source": [
    "Use asynchronous programming with asyncio to allow embedding generation to run in the background without blocking other operations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4aefff-2ab7-4e11-a01d-7657c6427d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# Define an asynchronous function to generate embeddings in parallel\n",
    "async def generate_embeddings_async(docs, embeddings):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    futures = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        # Use loop.run_in_executor to run embedding generation asynchronously\n",
    "        futures.append(loop.run_in_executor(None, embeddings.embed_query, doc.page_content))\n",
    "    \n",
    "    # Asynchronously gather all results\n",
    "    embedded_docs = await asyncio.gather(*futures)\n",
    "    \n",
    "    return embedded_docs\n",
    "\n",
    "# Function to regenerate document search database asynchronously\n",
    "async def regenerate_database_async(docs, embeddings):\n",
    "    print(\"Generating embeddings asynchronously...\")\n",
    "    \n",
    "    # Generate embeddings asynchronously\n",
    "    embedded_docs = await generate_embeddings_async(docs, embeddings)\n",
    "    \n",
    "    # Create a document search database using the embeddings\n",
    "    db = DocArrayInMemorySearch.from_embeddings(embedded_docs, docs)\n",
    "    \n",
    "    return db\n",
    "\n",
    "# Function to load or regenerate cache asynchronously\n",
    "async def load_or_regenerate_cache_async(docs, embeddings):\n",
    "    if is_cache_valid_with_notification(cache_file, document_file):\n",
    "        print(\"Cache is valid and not expired. Loading embeddings from cache.\")\n",
    "        return load_embeddings_cache_with_error_handling()\n",
    "    else:\n",
    "        print(\"Cache is invalid or expired. Regenerating embeddings asynchronously.\")\n",
    "        db = await regenerate_database_async(docs, embeddings)\n",
    "        save_embeddings_cache_with_error_handling(db)  # Save new cache\n",
    "        return db\n",
    "\n",
    "# Main function to handle asynchronous embedding generation\n",
    "async def main():\n",
    "    db = await load_or_regenerate_cache_async(docs, embeddings)\n",
    "    \n",
    "    # If db is None, ensure the program doesn't crash\n",
    "    if db is not None:\n",
    "        # Continue with similarity search and Q&A process using the cache or newly created embeddings\n",
    "        query = \"Please suggest a shirt with sunblocking\"\n",
    "        docs = db.similarity_search(query)\n",
    "        \n",
    "        # Display the first matching document\n",
    "        docs[0]\n",
    "    else:\n",
    "        print(\"Unable to proceed due to cache or document issues.\")\n",
    "\n",
    "# Run the asynchronous main function\n",
    "asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4297cd43-2a52-44cd-a9f8-23df009856b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04cc4583-b097-4fdc-944a-fbe55ace4409",
   "metadata": {},
   "source": [
    "Suggestions for further improvements:\n",
    "a. Combine asynchronous handling with real-time progress tracking to monitor embedding generation without blocking the application.\n",
    "b. Add retry mechanisms for failed asynchronous tasks to ensure reliable embedding generation even when facing network or resource issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ceacf2-a9ef-4c4a-93d1-017ebaac015c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1774264a-4221-40f2-ba1e-c14bc28490e7",
   "metadata": {},
   "source": [
    "Cell 20: Combine Asynchronous Handling with Real-Time Progress Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38a5ae2-e303-4a4e-92c7-dbd36cc7416b",
   "metadata": {},
   "source": [
    "Use tqdm to track the progress of embedding generation in real-time while running the task asynchronously, ensuring the process doesn't block other operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0b3957-1482-4281-ae88-51fa8197124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "# Define an asynchronous function to generate embeddings in parallel with real-time progress tracking\n",
    "async def generate_embeddings_async_with_progress(docs, embeddings):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    futures = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        # Use loop.run_in_executor to run embedding generation asynchronously\n",
    "        futures.append(loop.run_in_executor(None, embeddings.embed_query, doc.page_content))\n",
    "    \n",
    "    # Asynchronously gather all results while showing progress\n",
    "    embedded_docs = []\n",
    "    for result in tqdm_asyncio.as_completed(futures, desc=\"Generating embeddings asynchronously\", total=len(futures)):\n",
    "        embedded_docs.append(await result)\n",
    "    \n",
    "    return embedded_docs\n",
    "\n",
    "# Function to regenerate document search database asynchronously with progress tracking\n",
    "async def regenerate_database_async_with_progress(docs, embeddings):\n",
    "    print(\"Generating embeddings asynchronously with progress tracking...\")\n",
    "    \n",
    "    # Generate embeddings asynchronously with progress tracking\n",
    "    embedded_docs = await generate_embeddings_async_with_progress(docs, embeddings)\n",
    "    \n",
    "    # Create document search database using the embeddings\n",
    "    db = DocArrayInMemorySearch.from_embeddings(embedded_docs, docs)\n",
    "    \n",
    "    return db\n",
    "\n",
    "# Function to load or regenerate cache asynchronously with progress tracking\n",
    "async def load_or_regenerate_cache_async_with_progress(docs, embeddings):\n",
    "    if is_cache_valid_with_notification(cache_file, document_file):\n",
    "        print(\"Cache is valid and not expired. Loading embeddings from cache.\")\n",
    "        return load_embeddings_cache_with_error_handling()\n",
    "    else:\n",
    "        print(\"Cache is invalid or expired. Regenerating embeddings asynchronously with progress tracking.\")\n",
    "        db = await regenerate_database_async_with_progress(docs, embeddings)\n",
    "        save_embeddings_cache_with_error_handling(db)  # Save new cache\n",
    "        return db\n",
    "\n",
    "# Main function to handle asynchronous embedding generation with progress tracking\n",
    "async def main_with_progress():\n",
    "    db = await load_or_regenerate_cache_async_with_progress(docs, embeddings)\n",
    "    \n",
    "    # If db is None, ensure the program doesn't crash\n",
    "    if db is not None:\n",
    "        # Continue with similarity search and Q&A process using the cache or newly created embeddings\n",
    "        query = \"Please suggest a shirt with sunblocking\"\n",
    "        docs = db.similarity_search(query)\n",
    "        \n",
    "        # Display the first matching document\n",
    "        docs[0]\n",
    "    else:\n",
    "        print(\"Unable to proceed due to cache or document issues.\")\n",
    "\n",
    "# Run the asynchronous main function with progress tracking\n",
    "asyncio.run(main_with_progress())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34961b05-5ce9-4020-97a1-007f31e8b1f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a27536e6-b9a1-4d26-bafb-86fd634f2e4f",
   "metadata": {},
   "source": [
    "Cell 21: Add Retry Mechanisms for Failed Asynchronous Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f894efb3-6c75-44a9-b597-35aa292460d1",
   "metadata": {},
   "source": [
    "Add a retry mechanism using tenacity to automatically retry failed embedding generation tasks, ensuring robustness in the face of network or resource issues.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62947c-1f28-4496-bd79-3d4d86f28030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tenacity for retry mechanisms\n",
    "# !pip install tenacity\n",
    "\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "\n",
    "# Function to retry failed embedding generation up to 3 times with a fixed wait time\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_fixed(2))\n",
    "async def generate_single_embedding_with_retry(doc, embeddings):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    # Use loop.run_in_executor to run embedding generation asynchronously\n",
    "    return await loop.run_in_executor(None, embeddings.embed_query, doc.page_content)\n",
    "\n",
    "# Define an asynchronous function to generate embeddings with retry mechanisms\n",
    "async def generate_embeddings_async_with_retries(docs, embeddings):\n",
    "    futures = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        # Generate embeddings with retry mechanism\n",
    "        futures.append(generate_single_embedding_with_retry(doc, embeddings))\n",
    "    \n",
    "    # Asynchronously gather all results while showing progress\n",
    "    embedded_docs = []\n",
    "    for result in tqdm_asyncio.as_completed(futures, desc=\"Generating embeddings with retries\", total=len(futures)):\n",
    "        embedded_docs.append(await result)\n",
    "    \n",
    "    return embedded_docs\n",
    "\n",
    "# Function to regenerate document search database asynchronously with retry mechanism\n",
    "async def regenerate_database_async_with_retries(docs, embeddings):\n",
    "    print(\"Generating embeddings asynchronously with retry mechanism...\")\n",
    "    \n",
    "    # Generate embeddings asynchronously with retries\n",
    "    embedded_docs = await generate_embeddings_async_with_retries(docs, embeddings)\n",
    "    \n",
    "    # Create document search database using the embeddings\n",
    "    db = DocArrayInMemorySearch.from_embeddings(embedded_docs, docs)\n",
    "    \n",
    "    return db\n",
    "\n",
    "# Function to load or regenerate cache asynchronously with retries\n",
    "async def load_or_regenerate_cache_async_with_retries(docs, embeddings):\n",
    "    if is_cache_valid_with_notification(cache_file, document_file):\n",
    "        print(\"Cache is valid and not expired. Loading embeddings from cache.\")\n",
    "        return load_embeddings_cache_with_error_handling()\n",
    "    else:\n",
    "        print(\"Cache is invalid or expired. Regenerating embeddings asynchronously with retry mechanism.\")\n",
    "        db = await regenerate_database_async_with_retries(docs, embeddings)\n",
    "        save_embeddings_cache_with_error_handling(db)  # Save new cache\n",
    "        return db\n",
    "\n",
    "# Main function to handle asynchronous embedding generation with retry mechanism\n",
    "async def main_with_retries():\n",
    "    db = await load_or_regenerate_cache_async_with_retries(docs, embeddings)\n",
    "    \n",
    "    # If db is None, ensure the program doesn't crash\n",
    "    if db is not None:\n",
    "        # Continue with similarity search and Q&A process using the cache or newly created embeddings\n",
    "        query = \"Please suggest a shirt with sunblocking\"\n",
    "        docs = db.similarity_search(query)\n",
    "        \n",
    "        # Display the first matching document\n",
    "        docs[0]\n",
    "    else:\n",
    "        print(\"Unable to proceed due to cache or document issues.\")\n",
    "\n",
    "# Run the asynchronous main function with retry mechanism\n",
    "asyncio.run(main_with_retries())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1257f4dd-491c-4343-994d-ead72b73f8b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87e8734c-c5a4-43a1-bae2-5d6f175dc1b8",
   "metadata": {},
   "source": [
    "Suggestions for further improvements:\n",
    "a. Implement more advanced retry policies (e.g., exponential backoff) to handle different types of failures more gracefully.\n",
    "b. Introduce concurrent task management to limit the number of parallel tasks to avoid overwhelming system resources in low-memory environments.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb9599e-dc02-4729-b741-a181546484d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828f9d35-dfac-4fcd-bcc0-b16982aa73d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53d7949-8b97-4274-b740-3f9c84fad99b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0434ac83-eaba-42ae-aff0-0922b3180153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e75a55-560d-41d6-a1aa-909e2830382c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c124bc8d-68ae-4e3e-942d-19a7d752e027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc20e761-3ba4-4f56-a4e1-f5d6600b5348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22c129a-178c-4fab-9477-5dbfbb627bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9d81c6-b409-4ab8-9bf9-e4225c188fec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ca7c04-bbca-48db-a7b2-5db59ada981e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
