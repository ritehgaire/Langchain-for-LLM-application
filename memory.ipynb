{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f3fffd4-d550-47ad-a6d5-942162171147",
   "metadata": {},
   "source": [
    "Cell 1: Setup the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da932b7-c95b-4ba2-8d48-1a315634f6b4",
   "metadata": {},
   "source": [
    "Set up the environment by loading environment variables, suppressing warnings, and preparing for model deprecation management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce03b216-0150-400a-bc32-65fd7601550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "_ = load_dotenv(find_dotenv())  # Load .env file\n",
    "\n",
    "# Suppress warnings related to deprecated functions or modules\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd77b23-92a6-4f63-9c46-c208186dffb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7faf524e-6003-4288-b544-86317e18f83b",
   "metadata": {},
   "source": [
    "Cell 2: Handle LLM model deprecation based on the current date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e3691a-85c2-4c1b-a4ad-0e344e83fbf1",
   "metadata": {},
   "source": [
    "Select the correct LLM model based on the current date, switching to a newer model after a specified target date.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a386fa-71e5-4e3c-af03-77e3308ab34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datetime module to handle date-based model selection\n",
    "import datetime\n",
    "\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the cutoff date after which the newer model is used\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Conditionally set the LLM model based on whether the current date is before or after the target date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf7a383-3777-49f7-bd39-9678aed014ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "fd7162d1-e281-4917-8f4f-d869736e8182",
   "metadata": {},
   "source": [
    "Cell 3: Setup Conversation with ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6605351b-e26d-42af-ba38-f4f8340ae418",
   "metadata": {},
   "source": [
    "Create a conversation chain using LangChain's ConversationBufferMemory, which stores the full conversation history for future use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704df9dc-21eb-4741-895f-350ee6dce337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary classes from LangChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Initialize the LLM with the selected model and zero temperature (deterministic output)\n",
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "\n",
    "# Initialize ConversationBufferMemory to store the entire conversation history\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Create a conversation chain that uses the initialized LLM and memory\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=memory, \n",
    "    verbose=True  # Enable detailed output of conversation processing\n",
    ")\n",
    "\n",
    "# Simulate a conversation with three turns\n",
    "conversation.predict(input=\"Hi, my name is Andrew\")\n",
    "conversation.predict(input=\"What is 1+1?\")\n",
    "conversation.predict(input=\"What is my name?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8f2dce-6108-407f-af51-9e7bb2fa1e10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c242bcb-c1e8-44a1-9784-e1f205cea3a7",
   "metadata": {},
   "source": [
    "Cell 4: Inspect and manipulate ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc08546a-a432-4d37-83d5-a91828654f7a",
   "metadata": {},
   "source": [
    "Demonstrate how to inspect and manipulate the conversation buffer by saving context and loading variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c10ce3-90d0-4a6d-948b-be0fb65f11ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the buffer which contains the full conversation history\n",
    "print(memory.buffer)\n",
    "\n",
    "# Load memory variables, this can be used to retrieve saved conversation context\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "# Reset memory and manually save a new conversation context\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"What's up\"})\n",
    "print(memory.buffer)  # Print the buffer after saving new context\n",
    "\n",
    "# Save another conversation context to the memory and inspect the buffer again\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})\n",
    "print(memory.buffer)  # Print the updated buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c5b34a-badf-45ac-b04c-a41b1a64a5b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c623e1bb-ccdc-4fda-b115-df80478b9627",
   "metadata": {},
   "source": [
    "Cell 5: Using ConversationBufferWindowMemory to store only recent conversations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e78ffb-a949-4dd9-944f-d26d093a47ec",
   "metadata": {},
   "source": [
    "Use ConversationBufferWindowMemory to store only the most recent k number of conversation exchanges, rather than the entire conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7edf909-5134-4eaa-ace0-662ba1ef0c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ConversationBufferWindowMemory class\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# Initialize ConversationBufferWindowMemory to store only the last k=1 exchanges\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "\n",
    "# Save a few conversation exchanges into the windowed memory\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})\n",
    "\n",
    "# Load memory variables to inspect what is stored (only stores the last `k=1` exchanges)\n",
    "print(memory.load_memory_variables({}))\n",
    "\n",
    "# Create a new conversation chain using this windowed memory\n",
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=memory, \n",
    "    verbose=False  # Disable verbose to suppress intermediate outputs\n",
    ")\n",
    "\n",
    "# Simulate a conversation using the windowed memory\n",
    "conversation.predict(input=\"Hi, my name is Andrew\")\n",
    "conversation.predict(input=\"What is 1+1?\")\n",
    "conversation.predict(input=\"What is my name?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd105fce-e57e-4a87-a4af-281c271a83ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5ce8009-19fa-4d72-9bda-cfca85aee5d0",
   "metadata": {},
   "source": [
    "Cell 6: Using ConversationTokenBufferMemory to limit token usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e2f220-f7e9-4fd6-a193-526592a7fb26",
   "metadata": {},
   "source": [
    "Use ConversationTokenBufferMemory to limit the number of tokens stored in memory, ensuring that only recent context is kept within the token limit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a025f8-fb52-48fb-aad8-933fb8d816c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ConversationTokenBufferMemory class from LangChain\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n",
    "# Initialize token-buffered memory with a maximum token limit\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
    "\n",
    "# Save several exchanges into memory, each contributing to the token count\n",
    "memory.save_context({\"input\": \"AI is what?!\"}, {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"}, {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, {\"output\": \"Charming!\"})\n",
    "\n",
    "# Load and inspect the memory, it should store only exchanges that fit within the 50-token limit\n",
    "print(memory.load_memory_variables({}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52221208-eb82-46e6-8aa8-409e3b6de645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b320f67-0212-4fd4-aaec-e282a9a1e1f3",
   "metadata": {},
   "source": [
    "Cell 7: Using ConversationSummaryBufferMemory to summarize conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c4579a-f8e3-4531-936e-8f0811a7dd9d",
   "metadata": {},
   "source": [
    "Use ConversationSummaryBufferMemory to summarize long conversations, retaining key points while limiting the token count to a manageable level.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a395bf82-ce84-4705-85b0-30ccd7fbab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ConversationSummaryBufferMemory class\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "# Create a long string that simulates a schedule for demonstration\n",
    "schedule = (\"There is a meeting at 8am with your product team. \"\n",
    "            \"You will need your powerpoint presentation prepared. \"\n",
    "            \"9am-12pm have time to work on your LangChain project which will go quickly \"\n",
    "            \"because Langchain is such a powerful tool. At Noon, lunch at the italian restaurant \"\n",
    "            \"with a customer who is driving from over an hour away to meet you to understand the latest in AI. \"\n",
    "            \"Be sure to bring your laptop to show the latest LLM demo.\")\n",
    "\n",
    "# Initialize summary buffer memory with a token limit\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "\n",
    "# Save several exchanges into the memory, including the detailed schedule\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, {\"output\": f\"{schedule}\"})\n",
    "\n",
    "# Load and inspect the summarized memory variables (summary of the conversation)\n",
    "print(memory.load_memory_variables({}))\n",
    "\n",
    "# Create a conversation chain using the summarized memory\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=memory, \n",
    "    verbose=True  # Enable verbose to show the summary in action\n",
    ")\n",
    "\n",
    "# Make a prediction to check how the memory summarization works\n",
    "conversation.predict(input=\"What would be a good demo to show?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315f3b9e-bbab-437c-b92e-d466eff98790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58b72bbb-e7e5-4d0c-b474-28d676afa6a1",
   "metadata": {},
   "source": [
    "Suggestions for further improvements:\n",
    "a. Add unit tests to validate each type of memory behavior.\n",
    "b. Implement persistence to save and load memory from files to maintain conversation history across sessions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
